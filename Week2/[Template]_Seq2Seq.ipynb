{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Template]-Seq2Seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOVNEG0Pkvx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdApFpGxbX6",
        "outputId": "e9efccdb-0984-4139-94d8-7b3b78360e42"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import unicodedata\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "# Download the file\r\n",
        "path_to_zip = tf.keras.utils.get_file(\r\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\r\n",
        "    extract=True)\r\n",
        "\r\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\r\n",
        "\r\n",
        "# Converts the unicode file to ascii\r\n",
        "def unicode_to_ascii(s):\r\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n",
        "      if unicodedata.category(c) != 'Mn')\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_sentence(w):\r\n",
        "  w = unicode_to_ascii(w.lower().strip())\r\n",
        "\r\n",
        "  # creating a space between a word and the punctuation following it\r\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\r\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\r\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\r\n",
        "\r\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\r\n",
        "\r\n",
        "  w = w.strip()\r\n",
        "\r\n",
        "  # adding a start and an end token to the sentence\r\n",
        "  # so that the model know when to start and stop predicting.\r\n",
        "  w = '<start> ' + w + ' <end>'\r\n",
        "  return w\r\n",
        "\r\n",
        "en_sentence = u\"May I borrow this book?\"\r\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\r\n",
        "print(preprocess_sentence(en_sentence))\r\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))\r\n",
        "\r\n",
        "# 1. Remove the accents\r\n",
        "# 2. Clean the sentences\r\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\r\n",
        "def create_dataset(path, num_examples):\r\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\r\n",
        "\r\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\r\n",
        "\r\n",
        "  return zip(*word_pairs)\r\n",
        "\r\n",
        "en, sp = create_dataset(path_to_file, None)\r\n",
        "def tokenize(lang):\r\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "      filters='')\r\n",
        "  lang_tokenizer.fit_on_texts(lang)\r\n",
        "\r\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\r\n",
        "\r\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\r\n",
        "                                                         padding='post')\r\n",
        "\r\n",
        "  return tensor, lang_tokenizer\r\n",
        "\r\n",
        "def load_dataset(path, num_examples=None):\r\n",
        "  # creating cleaned input, output pairs\r\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\r\n",
        "\r\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\r\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\r\n",
        "\r\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\r\n",
        "\r\n",
        "\r\n",
        "# Try experimenting with the size of that dataset\r\n",
        "num_examples = 30000\r\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\r\n",
        "\r\n",
        "# Calculate max_length of the target tensors\r\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\r\n",
        "\r\n",
        "# Creating training and validation sets using an 80-20 split\r\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\r\n",
        "\r\n",
        "# Show length\r\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n",
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wos4NR2ZIlIz"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\r\n",
        "BATCH_SIZE = 64\r\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\r\n",
        "embedding_dim = 256 # embedding size\r\n",
        "units = 1024 # size of hidden state\r\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\r\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzn9By64LZWH"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(units, return_state=True)\n",
        "\n",
        "  def call(self, input_sentence):\n",
        "    embedding_input_sentence = self.embedding(input_sentence)\n",
        "    output, hidden_h, hidden_c = self.lstm(embedding_input_sentence)\n",
        "    return [hidden_h, hidden_c]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBc-UzITDhp"
      },
      "source": [
        "encoder = Encoder(embedding_dim, units)\n",
        "input = tf.zeros((BATCH_SIZE, max_length_inp))\n",
        "# encoder(input)[0].shape, encoder(input)[1].shape\n",
        "thought_vector = encoder(input)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7vhUN9mTP0g"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(units, return_state=True)\n",
        "    self.fc = tf.keras.layers.Dense(units)\n",
        "\n",
        "  def call(self, x, last_state):\n",
        "    embedding_x = self.embedding(x)\n",
        "    output, hidden_h, hidden_c = self.lstm(embedding_x, initial_state=last_state)\n",
        "    return self.fc(hidden_h), [hidden_h, hidden_c]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2eylrdUGrE"
      },
      "source": [
        "decoder = Decoder(embedding_dim, units)\n",
        "decoder_input = tf.zeros((BATCH_SIZE, 1))\n",
        "output, updated_thought_vector = decoder(decoder_input, thought_vector)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "bVQpUsoKUYuV",
        "outputId": "636f967d-ba5f-4b1d-bd3a-5651eabfdf50"
      },
      "source": [
        "thought_vector.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-78fdd6401c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthought_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtwCemoLXoB1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}